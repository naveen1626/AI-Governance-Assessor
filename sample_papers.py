SAMPLE_PAPERS = [
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Revolutionizing CRISPR Technology with Artificial Intelligence",
    "abstract": "This review describes how artificial intelligence models, including deep learning and large language models, are being integrated into the CRISPR workflow to improve target selection, off-target prediction, editing efficiency, and experimental design automation across research and therapeutic applications.[web:118]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "CRISPR-GPT for Agentic Automation of Gene-Editing Experiments",
    "abstract": "The paper presents CRISPR-GPT, a multi‑agent large language model system that plans, executes, and analyzes CRISPR gene‑editing experiments end‑to‑end, showing that LLM agents can design guide RNAs, generate protocols, and iteratively refine experiments based on lab results.[web:88][web:85]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Artificial Intelligence-Based Genome Editing in CRISPR/Cas9",
    "abstract": "This article reviews AI‑based models such as DeepCRISPR, CRISTA, and DeepHF that learn sequence and genomic‑context features to predict on‑target efficiency and off‑target risks for CRISPR/Cas9, enabling more accurate guide RNA design for research and clinical genome editing.[web:119]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Advancing Genome Editing with Artificial Intelligence",
    "abstract": "The authors survey how machine learning and deep learning are used to model DNA–protein interactions, predict editing outcomes, and design editors and guides, and they outline key challenges in data quality, interpretability, and regulatory acceptance for AI‑enabled genome editing.[web:141]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "AI Meets CRISPR for Precise Gene Editing",
    "abstract": "This work combines a machine‑learning model with CRISPR base editing to predict and control editing outcomes at single‑nucleotide resolution, allowing more precise disease‑modeling mutations and suggesting routes toward safer gene therapies.[web:137][web:121]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Artificial Intelligence for CRISPR Guide RNA Design",
    "abstract": "The preprint proposes an AI framework that integrates sequence features, chromatin context, and predicted repair outcomes to design guide RNAs optimized for high on‑target activity and low off‑target effects, outperforming existing rule‑based and single‑task models on benchmark datasets.[web:134]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "CRISPR-tica.ai: A Function-Informed Generative Modeling Pipeline for Genome Editing",
    "abstract": "CRISPR-tica.ai uses generative modeling conditioned on functional readouts to propose novel CRISPR perturbations and guide sequences, enabling data‑driven exploration of sequence–function landscapes in gene regulation and therapeutic editing.[web:132]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Integrating Artificial Intelligence in Drug Discovery and Early Drug Development",
    "abstract": "This review discusses how AI supports target identification, virtual screening, de novo molecule design, and early clinical development, illustrating successful case studies where machine learning shortened timelines, reduced costs, or proposed candidates that progressed into trials.[web:122][web:128]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "AI-Driven Drug Discovery: A Comprehensive Review",
    "abstract": "The paper systematically summarizes deep learning methods for hit discovery, lead optimization, ADMET prediction, and polypharmacology modeling, highlighting industrial deployments of generative models and multi‑task predictors in small‑molecule drug discovery pipelines.[web:125][web:128]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Biomedical paper",
    "expected_category": "biomedical",
    "title": "Harnessing AI for Advancing Pathogenic Microbiology",
    "abstract": "This article reviews applications of machine learning to pathogen genomics, virulence and antibiotic‑resistance prediction, host–pathogen interaction modeling, and outbreak analysis, emphasizing how AI can augment laboratory microbiology and public‑health surveillance.[web:123]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design with Generative and Agentic AI",
    "abstract": "This survey synthesizes recent progress in applying generative AI, large language models, and agentic workflows across the digital electronic design automation (EDA) stack. It reviews how diffusion models, graph neural networks, and reinforcement learning are used for placement, routing, timing closure, and verification, and how LLM-based agents orchestrate legacy tools to achieve end‑to‑end autonomous design flows. The paper contrasts algorithm‑centric approaches, which replace heuristic engines with learned models, and agent‑centric approaches, which use LLMs to plan and invoke sequences of tool commands, and discusses emerging challenges in security, trust, and evaluation for highly autonomous EDA systems.[web:195]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "A Comprehensive Survey on Electronic Design Automation and Graph Neural Networks",
    "abstract": "This article presents an extensive review of how graph neural networks (GNNs) are being integrated into electronic design automation workflows. It categorizes applications of GNNs across logic synthesis, floorplanning, placement, routing, timing and power analysis, and design for manufacturability, emphasizing how circuit netlists and layouts can be represented as graphs or hypergraphs. The survey compares model architectures, datasets, and benchmarks, and highlights open problems such as generalization across technology nodes, interpretability of learned graph embeddings, and integration of GNN‑based predictors into industrial sign‑off flows.[web:197]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "Human-in-the-Loop AI EDA for Chip Design",
    "abstract": "This paper proposes a human‑in‑the‑loop AI framework for EDA that embeds expert feedback into a modular learning system for chip design. The architecture combines machine learning models with designer annotations to iteratively refine optimization strategies for tasks such as placement, routing, and timing closure. Experiments show that incorporating human guidance improves convergence, reduces undesirable tool behaviors, and increases trust in AI‑assisted design workflows compared with fully automated black‑box optimization.[web:198]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence: GPUs, ASICs, and FPGAs",
    "abstract": "This survey analyzes how modern computer architectures have co‑evolved with deep learning workloads and provides a structured comparison of major AI accelerator platforms. It examines GPUs, AI‑specific ASICs, and FPGAs in terms of dataflow organization, memory hierarchy design, sparsity and quantization support, and energy–performance trade‑offs. The paper also discusses emerging paradigms such as processing‑in‑memory and neuromorphic computing, arguing that hardware–software co‑design has become essential for sustaining AI performance scaling.[web:203]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "A Unified AI Accelerator Interface for Scalable RISC‑V Architectures",
    "abstract": "This work introduces a flexible, open interface for integrating AI accelerators into RISC‑V‑based systems. The proposed interface natively supports vector, matrix, and tensor extensions and unifies diverse data‑access paths including L1 cache, system bus, coprocessor links, and inter‑accelerator communication to accommodate different AI dataflow patterns. It further defines a virtual‑memory interface that allows accelerators to obtain physical addresses via private TLBs or direct MMU access and supports decoupled microarchitectures, improving portability and scalability of heterogeneous RISC‑V AI systems.[web:200]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "AI Hardware Accelerators: Architecture Trade‑offs, Performance, and Deployment Considerations",
    "abstract": "This paper provides a detailed analysis of AI hardware accelerator architectures and the trade‑offs that differentiate classes such as GPU‑like cores, systolic arrays, and custom ASIC fabrics. It evaluates performance, power, and area characteristics across representative deep neural network workloads, and discusses practical deployment issues including programmability, compiler support, thermal limits, and integration into existing datacenter and edge platforms. By combining quantitative benchmarks with qualitative design guidance, the work aims to inform architects and practitioners choosing or designing AI accelerators for production use.[web:206]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "Meta’s Second Generation AI Chip: Model–Chip Co‑Design and System Integration",
    "abstract": "This paper describes the architecture and deployment of Meta’s second‑generation in‑house AI accelerator and argues for tight model–chip co‑design. It details the chip’s compute array, memory subsystem, and interconnect, and explains how the accelerator is integrated into a larger system with host CPUs, high‑bandwidth DRAM, and network interfaces. The authors present performance and efficiency results on large‑scale recommendation and generative AI workloads, showing significant cost and energy savings compared with off‑the‑shelf solutions and highlighting lessons for future datacenter AI chip designs.[web:209]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "AI in Real‑World Chip Design Workflows: A Technical Overview",
    "abstract": "This technical overview explains how AI techniques are being embedded into practical chip design workflows from architecture exploration through physical implementation. It describes the use of machine‑learning surrogate models and reinforcement learning for design space exploration, AI‑assisted RTL development, and deep reinforcement learning for macro placement and layout optimization. Case studies, including NVIDIA’s use of RL for circuit design and commercial tools like DSO.ai and Cerebrus, demonstrate that AI can achieve better power‑performance‑area (PPA) results in less time than traditional heuristic flows, while also enabling knowledge transfer between projects.[web:207][web:204]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "Why Chip Design Needs Industrial‑Grade EDA AI",
    "abstract": "This article argues that scaling AI in chip design requires industrial‑grade infrastructure for reliability, observability, and governance. It describes how AI models participate in tasks such as placement, routing, ECO optimization, and sign‑off analysis, and emphasizes the need for robust logging, verification, and fallback mechanisms when AI systems evaluate thousands of candidate layouts or optimization runs. The paper outlines design principles for trustworthy EDA AI, including reproducibility, explainability of suggestions, and integration with existing verification and regression frameworks.[web:205]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Semiconductor paper",
    "expected_category": "semiconductor",
    "title": "AI in Chip Design: Compressing Timelines and Expanding Innovation Horizons",
    "abstract": "This article reviews how AI‑driven EDA tools are transforming chip design at advanced process nodes such as 5 nm and 3 nm. It highlights commercial platforms like Cadence Cerebrus and Synopsys DSO.ai that use reinforcement learning and evolutionary strategies to automatically explore design configurations and optimize PPA. The discussion includes real‑world examples where AI‑guided flows reduced design cycle time, improved frequency and power metrics, and allowed engineering teams to manage exploding complexity in leading‑edge semiconductor products.[web:204][web:207]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Scaling Laws for Differentially Private Language Models",
    "abstract": "Scaling laws are an important tool for predicting how large language model performance improves with additional parameters, data, and compute, but existing work largely ignores the effects of differential privacy. This paper derives and empirically validates scaling laws tailored to differentially private (DP) language model training, capturing how privacy noise and clipping interact with model and dataset size. The authors present a framework that characterizes compute–privacy–utility trade‑offs, identify optimal training configurations under DP constraints, and show that their laws accurately forecast performance across a wide range of training regimes, enabling principled planning of DP LLM training runs.[web:215]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Unified Scaling Laws for Compressed Representations",
    "abstract": "As large language models grow, practitioners increasingly rely on compression strategies such as quantization and sparsity to reduce training and inference costs, yet the interplay between scaling and compression remains poorly understood. This work proposes a unified scaling law formulation that predicts model performance when training on compressed representations, including sparse, scalar‑quantized, sparse‑quantized, and vector‑quantized formats. Through extensive experiments, the authors demonstrate that their scaling framework accurately models the impact of different compression schemes on loss and task performance, providing guidance for jointly choosing model scale and compression strategy under compute constraints.[web:218]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Explaining Neural Scaling Laws",
    "abstract": "Deep neural networks often exhibit precise power‑law relationships between population loss, dataset size, and model parameters, but the mechanisms behind these scaling laws have remained elusive. This paper develops a theoretical framework that identifies four distinct scaling regimes—variance‑limited and resolution‑limited behavior for both model size and dataset size—and analytically explains when each regime arises. The theory matches empirical observations across diverse architectures and tasks, clarifies why extrapolating along the right scaling frontier matters, and offers practical insight into how to allocate parameters and data for maximal performance gains.[web:221]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "A Survey of Scaling Law Fitting Techniques in Deep Learning",
    "abstract": "Modern foundation model development relies heavily on scaling laws to forecast performance and guide architecture and hyperparameter choices, yet the methodology used to fit these laws varies widely. This survey systematically reviews functional forms for scaling laws, experimental design choices for collecting scaling data, and optimization methods used to fit scaling relations. The authors compare techniques across synthetic and real training curves, highlight common pitfalls such as extrapolation errors and mis‑specified noise models, and provide practical recommendations for reliably using scaling laws in large‑scale model training.[web:224]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Apple Intelligence Foundation Language Models Tech Report 2025",
    "abstract": "This technical report introduces two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services. The authors describe the model architectures, including a compact on‑device transformer and a larger mixture‑of‑experts server model, as well as the training data mixture, alignment methods, and safety filters. They present benchmarks on language understanding, generation, and multimodal tasks, and show that careful model–hardware co‑design enables low‑latency, privacy‑preserving deployment of powerful generative models on consumer hardware at scale.[web:219][web:222]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "State of Foundation Model Training Report 2025",
    "abstract": "This report analyzes how organizations are actually training large foundation models in practice, based on interviews and survey data from industry teams. It highlights trends such as increasing model and dataset sizes, a shift toward on‑premises GPU clusters for predictable capacity, and a strong focus on data and software engineering over pure modeling. The report discusses emerging best practices for proof‑of‑concept projects, evaluation strategies, and infrastructure design, and argues that simply scaling models is insufficient without careful attention to data quality, training efficiency, and downstream application fit.[web:216]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Large Language Model Training in 2026: Architectures, Infrastructure, and Workflows",
    "abstract": "This article provides a practical guide to training large language models, structured around four stages: data collection and preprocessing, model configuration, training, and deployment. It covers transformer architectures with mixture‑of‑experts extensions, discusses key hyperparameters such as number of experts, layers, and attention heads, and explains how tools like automated sweeps are used to search the configuration space efficiently. The authors describe distributed training patterns including data, pipeline, and tensor parallelism, highlight memory‑optimization techniques like ZeRO and FlashAttention, and outline the economic trade‑offs between full pretraining and fine‑tuning existing models.[web:214]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Training Large Language Models on Narrow Tasks Can Lead to Broad Capabilities",
    "abstract": "This Nature paper investigates whether training large language models on narrowly defined task distributions can nevertheless yield broad general‑purpose capabilities. The authors train models on restricted domains and evaluate them on a wide range of benchmarks, showing that even narrow training often leads to emergent abilities that extend beyond the training tasks. They analyze factors such as model scale, data diversity within the narrow domain, and inductive biases, and discuss implications for safety, alignment, and governance of models whose capabilities may exceed what their training objectives suggest.[web:223]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "Apple’s On‑Device and Server Foundation Models: Architecture and Deployment in Apple Intelligence",
    "abstract": "This research article outlines the design of Apple’s generative models that underpin Apple Intelligence, focusing on both on‑device and server‑side architectures. It introduces a roughly 3‑billion‑parameter on‑device model optimized for Apple silicon and a larger mixture‑of‑experts model tailored for privacy‑preserving cloud inference. The paper details architectural choices that improve tool use, reasoning, multilingual support, and multimodal understanding, and presents latency and quality measurements demonstrating that the combined system can deliver powerful AI features while preserving user privacy and resource efficiency.[web:219]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Ai_ml paper",
    "expected_category": "ai_ml",
    "title": "The State of LLMs 2025: Progress, Problems, and Predictions",
    "abstract": "This 2025 review synthesizes the state of large language models, covering advances in architectures, training techniques, and evaluation. It discusses notable models such as DeepSeek R1 and reinforcement‑learning‑from‑verifiable‑reward (RLVR) systems, as well as trends like inference‑time scaling, tool‑augmented reasoning, and specialized evaluation benchmarks. The author also analyzes open problems in robustness, hallucinations, and data curation, and offers predictions about the evolution of training methods, model specialization, and regulatory pressures in the coming years.[web:210]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "Enhancing Open-Access Language Model Safety by Connection Pruning",
    "abstract": "Large language models are highly vulnerable to jailbreak attacks that bypass safety measures and elicit harmful outputs. This paper introduces Nexus Scissor, a connection-pruning framework that removes safety-critical neuron connections identified as mediating harmful behavior. Evaluated against four strong white-box jailbreak families, including AutoDAN, GenExploit, bad-demonstration finetuning, and template-based prompts, Nexus Scissor reduces the average attack success rate by over 91% across several open models, with up to 95.5% reduction on LLaMA‑2‑7B. The results show that targeted structural pruning can substantially harden aligned open-access LLMs against a broad range of adversarial jailbreak techniques while preserving benign capabilities.[web:226]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "Understanding the Adversarial Landscape of Large Language Models",
    "abstract": "This article organizes the security threats to large language models into four overarching objectives: privacy breaches, integrity violations, availability disruption, and abuse of capabilities. It surveys concrete attack techniques such as prompt injection, data exfiltration via conversational interfaces, content-filter evasion, and tool‑enabled escalation, and maps them to real-world deployment contexts. The authors argue that for LLMs, model behavior itself is the attack surface, and they call for security evaluations that focus on exploitability and emergent behavior under adversarial inputs rather than solely on static vulnerabilities in code or infrastructure.[web:227][web:225]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "The 2026 State of LLM Security: Key Findings and Benchmarks",
    "abstract": "This report examines how security incidents involving large language models increasingly stem from emergent behaviors rather than discrete software bugs. It analyzes failure modes such as prompt injection, context manipulation, and unsafe tool invocation, and argues that attackers are learning to weaponize how models interpret instructions and assemble context. The authors propose a benchmarking approach for LLM security that emphasizes testing models under adversarial input, observing decision‑making in realistic workflows, validating safeguards under changing context, and detecting regressions over time, positioning behavior‑focused evaluation as central to managing LLM risk at scale.[web:225]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "A Framework for Evaluating Emerging Cyberattack Capabilities of AI",
    "abstract": "This paper introduces a data-driven framework to evaluate how AI systems can enhance cyberattack capabilities along the full attack chain. Building on over 12,000 real-world cyber incidents and curated attack archetypes, the authors adapt existing cyber kill-chain models to highlight phases where AI can most amplify offensive operations. Through bottleneck analysis and expert studies, they identify critical stages—such as reconnaissance, vulnerability discovery, and exploitation—where AI assistance could dramatically reduce attacker cost and time, and they recommend prioritized defensive investments and evaluation tasks for monitoring AI-enabled offensive potential.[web:230]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "NIST AI 100‑2 E2025: Adversarial Machine Learning – A Taxonomy and Terminology of Attacks and Mitigations",
    "abstract": "The 2025 NIST AI 100‑2 report significantly expands the taxonomy of adversarial machine learning attacks and defenses. Going beyond classic categories like evasion, poisoning, and privacy attacks, it introduces refined subcategories that capture practical threats to modern ML and generative models, including prompt injection, data‑supply chain attacks, and agent vulnerabilities. The updated guidance emphasizes real‑world enterprise deployment risks and GenAI‑specific issues, and provides practitioners with clearer terminology, structured attack categorizations, and high‑level mitigation strategies aligned with trustworthy and responsible AI objectives.[web:231][web:234]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "The New Shockwave: AI‑Generated Cyber Attacks",
    "abstract": "This paper analyzes a new wave of AI‑generated cyber attacks and their impact across personal privacy, finance, healthcare, cryptocurrency, and national security. Drawing on 2024–2025 incident data, it quantifies rising economic damages and highlights a widening AI governance gap, noting that the vast majority of organizations suffering AI-related incidents lacked adequate access controls. The authors describe how attackers use generative models and autonomous agents for tasks such as phishing content creation, exploit development, cryptojacking via vulnerabilities in AI frameworks, and targeting critical infrastructure, underscoring the urgency of integrating AI-aware defenses into cyber risk management.[web:233]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "Cybersecurity Challenges in the Era of AI",
    "abstract": "This study surveys how the proliferation of artificial intelligence is reshaping the cyber threat landscape, focusing on automated phishing, deepfake-enabled identity fraud, and machine learning–specific attacks. It documents the growth of AI‑driven cyber attacks that leverage automation, speed, and adaptability to evade traditional defenses, noting that AI systems can learn from previous attempts and dynamically alter their behavior. The paper also discusses defensive applications of AI and concludes with recommendations for organizations to update security strategies, workforce skills, and governance to cope with increasingly AI‑augmented adversaries.[web:239]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "Adversarial Machine Learning is Fighting Back: Interpreting NIST’s 2025 Guidelines",
    "abstract": "This article explains the finalized 2025 NIST guidelines on adversarial machine learning and their implications for practitioners. It reviews the main attack types and life‑cycle stages identified by NIST, including how adversaries can manipulate training data, model inputs, and deployed systems to compromise integrity, availability, or confidentiality. The discussion highlights recommended mitigation approaches, such as robust training, monitoring for distribution shifts and adversarial patterns, and integrating adversarial‑resilience considerations into the broader AI system development life cycle.[web:231]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "AI and the 2026 Threat Landscape",
    "abstract": "This report describes how AI has become embedded into every stage of the cyberattack lifecycle, from initial access to lateral movement and impact. It details how generative models enable hyper‑personalized, multilingual phishing, realistic voice cloning, and deepfake‑based social engineering, while autonomous agents perform rapid reconnaissance to map networks, discover high‑value assets, and locate misconfigurations. The authors argue that defenders must pair AI‑driven detection with stronger organizational preparedness and governance to counter increasingly automated and adaptive adversaries.[web:229]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Cybersecurity paper",
    "expected_category": "cybersecurity",
    "title": "G7 Cyber Expert Group Statement on AI and Cybersecurity 2025",
    "abstract": "This G7 statement outlines policy-level concerns and scenarios where AI can both empower attackers and support defenders in cyberspace. It highlights AI‑powered phishing and impersonation, automated exploit development using reinforcement learning, and adaptive malware that evolves to evade detection as key emerging threats. At the same time, the document encourages responsible AI use for threat detection and incident response, and calls for international cooperation, risk management frameworks, and information sharing to address AI‑enabled cyber risks to financial systems and critical infrastructure.[web:236]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Machine Learning for Accelerating Energy Materials Discovery",
    "abstract": "This perspective highlights how machine learning is transforming the discovery of sustainable energy materials, including battery electrodes, catalysts, and solid electrolytes. By integrating quantum‑accurate simulations, high‑throughput experiments, and data‑driven models, the authors show how ML can rapidly predict key properties such as band gaps, ionic conductivity, and catalytic activity across vast materials spaces. They discuss successful case studies, outline best practices for building reliable models, and emphasize the importance of curated datasets and uncertainty quantification for deploying ML in practical materials discovery workflows.[web:4]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "De Novo Multi‑Objective Generation Framework for Energetic Materials",
    "abstract": "This work presents a de novo molecular design framework for energetic materials that simultaneously optimizes detonation performance and bond dissociation energy as a proxy for stability. The pipeline couples a deep learning–based generative model, pre‑trained and then fine‑tuned via transfer learning on a curated dataset of synthesized energetic molecules, with machine‑learning property predictors and Pareto‑front multi‑objective screening. High‑precision quantum mechanical calculations and synthetic accessibility assessment are then used to validate recommended candidates, yielding novel energetic molecules that strike improved trade‑offs between energy output and safety compared with known compounds.[web:6]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "De Novo Design of Energetic Molecules by Coupling Multiple Deep Learning Methods",
    "abstract": "This paper develops a comprehensive de novo design strategy for energetic molecules by coupling several deep learning approaches, including graph neural networks, variational autoencoders, and generative adversarial networks. The authors construct generative models that explore chemically valid energetic backbones and attach functional groups to tune detonation performance, density, and impact sensitivity. Extensive in silico screening identifies promising candidates that outperform reference explosives on predicted performance metrics while maintaining acceptable safety margins, demonstrating the power of multi‑model deep learning for targeted energetic materials discovery.[web:18]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Artificial Intelligence Empowered New Materials: Discovery, Synthesis, Prediction and Validation",
    "abstract": "This 2026 review surveys recent advances in AI‑powered materials research across discovery, synthesis, property prediction, and experimental validation. It describes how machine learning models are used to predict formability and phase stability, infer mechanical and electronic properties, and guide experimental synthesis conditions in domains such as alloys, energy materials, and functional oxides. The authors discuss design strategies for enhancing AI performance, including data processing, algorithmic advances, and automated laboratory platforms, and offer an outlook on how self‑improving AI systems can reshape materials research and development.[web:246]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Accelerating Materials Discovery via AI‑Agent Integration of Large‑Scale Simulations and Databases",
    "abstract": "This article introduces an AI‑agent framework that integrates large language models with high‑throughput computing, multiscale simulations, and materials databases to accelerate discovery. The system uses specialized agents for tasks like text‑to‑data extraction, synthesis planning, and workflow construction, enabling closed‑loop design that links ab initio calculations, molecular dynamics, and phase‑field simulations with machine‑learning surrogates. The Matty platform exemplifies this approach by orchestrating perception, memory, decision‑making, execution, and learning modules, allowing autonomous exploration of molecular and crystalline materials spaces for targeted properties.[web:244]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Machine Learning‑Driven Materials Discovery: Unlocking Next‑Generation Materials",
    "abstract": "This review highlights real‑world applications of automated, machine‑learning‑driven approaches for predicting mechanical, thermal, electrical, and optical properties across metals, polymers, ceramics, and composites. It surveys supervised and unsupervised models that map composition and structure to properties, along with active‑learning and Bayesian‑optimization schemes that iteratively select new experiments or simulations. The authors argue that ML‑driven workflows can uncover unconventional compositions and microstructures that would be difficult to identify using traditional trial‑and‑error experimentation, thereby unlocking next‑generation materials for energy, structural, and electronic applications.[web:13]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Toward High‑Entropy Material Discovery for Energy Applications with Machine Learning",
    "abstract": "High‑entropy materials offer vast compositional design spaces for energy applications, but their exploration is severely constrained by experimental cost. This paper combines machine learning with computational screening to prioritize promising high‑entropy compositions for properties such as stability, ionic conductivity, and catalytic activity. By training models on available experimental and simulated data and using them to guide targeted calculations, the workflow efficiently proposes candidate high‑entropy systems for batteries, thermoelectrics, and catalysis, illustrating how ML can accelerate discovery in complex multi‑component materials spaces.[web:241]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Advancing Materials Discovery Through Artificial Intelligence",
    "abstract": "This article surveys how artificial intelligence is being applied along the full materials discovery pipeline, from design and synthesis planning to reaction condition optimization and scale‑up. It discusses ML models for retrosynthetic route planning, prediction of reaction yields and selectivities, and optimization of synthesis parameters using Bayesian and reinforcement learning. The authors also cover AI‑enabled analysis of high‑throughput experiments and propose that combining data‑centric AI with domain knowledge can significantly shorten the time from concept to deployable materials and chemicals.[web:248]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "AI‑Accelerated Materials Discovery: How Generative Models, Graph Neural Networks, and Autonomous Labs Are Reshaping R&D",
    "abstract": "This 2025 overview describes AI‑accelerated materials discovery as the use of generative models, graph neural networks, and self‑driving laboratories to predict, design, and synthesize new materials. It explains how graph‑based models encode crystal and molecular structures, how generative models propose candidates with tailored properties, and how robotic platforms carry out high‑throughput synthesis and characterization in closed‑loop fashion. Case studies from energy storage, catalysis, and semiconductors illustrate orders‑of‑magnitude reductions in iteration time compared with conventional workflows.[web:240][web:243]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Chemistry paper",
    "expected_category": "chemistry",
    "title": "Physics‑Informed AI Excels at Large‑Scale Discovery of New Materials",
    "abstract": "This work reports a physics‑informed AI framework that dramatically speeds up large‑scale discovery of new materials by embedding physical constraints and domain knowledge into machine‑learning models. By training on Materials Project and related databases and enforcing structure‑property relationships derived from solid‑state physics, the system constructs an AI‑powered materials map that screens candidate compounds for stability and functional properties. The approach achieves high accuracy while reducing the need for expensive quantum calculations, enabling rapid identification of promising materials for further computational and experimental validation.[web:249][web:247]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "ReactorNet: A Machine Learning Framework to Identify Control Rod Patterns from Thermal Neutron Flux Images in Nuclear Reactors",
    "abstract": "This study introduces ReactorNet, a machine learning framework for monitoring pressurized water reactors by analyzing thermal neutron flux distributions under different operating conditions.[web:254] Using images derived from ex‑core neutron detector responses, the model learns patterns associated with control rod positions, reactivity, coolant temperature, boron concentration, and coolant density, enabling sensitive detection of core state changes.[web:254] The authors show that the approach can automate analysis of large flux datasets, reduce operator workload and human error, and improve anomaly detection and prediction of future reactor conditions, representing a shift toward data‑driven reactor core monitoring.[web:254]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Using AI to Monitor Inaccessible Locations of Nuclear Energy Systems with Virtual Sensors",
    "abstract": "This work proposes advanced machine‑learning‑based virtual sensors to monitor thermal and flow conditions in hard‑to‑reach or harsh regions of nuclear energy systems in real time.[web:255] By training models on simulation and limited physical measurements, the virtual sensors infer critical parameters without installing physical instrumentation everywhere, providing a continuous “virtual map” of reactor operation.[web:255] The approach enhances safety and efficiency by enabling earlier detection of abnormal conditions, improving monitoring accuracy, and reducing the need for intrusive measurements in high‑radiation environments.[web:255]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Reinforcement Learning for Nuclear Microreactor Control",
    "abstract": "This paper investigates the use of deep reinforcement learning (RL) to control power output in nuclear microreactors through load‑following operation.[web:258] In simulated environments, RL agents adjust control drum positions based on reactor feedback and learn policies that achieve similar or better power‑tracking performance than conventional PID controllers, while reducing control effort by up to 150% in imperfect sensor and fluctuating condition scenarios.[web:258] The work also explores multi‑agent RL variants that train faster than single‑agent approaches, suggesting a path toward future digital twins where RL drives autonomous microreactor control subject to extensive validation and safety constraints.[web:258]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Physics‑Enhanced Machine Learning for Probabilistic Risk Assessment of Nuclear Power Plants",
    "abstract": "This article reviews recent implementations of physics‑enhanced machine learning (PEML) techniques in probabilistic risk assessment for nuclear power plants.[web:251] By embedding physical models and constraints into data‑driven algorithms, PEML improves prediction of rare events, component failures, and system responses under off‑normal conditions where purely statistical models struggle.[web:251] The paper surveys applications in reliability analysis, severe accident modeling, and dynamic risk estimation, and argues that combining physics‑based simulation with ML offers a promising route to more accurate and computationally efficient nuclear safety assessments.[web:251]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "AI‑Enabled Monitoring and Digital Twinning for Pyroprocessing in Advanced Nuclear Fuel Cycles",
    "abstract": "This ARPA‑E project report outlines an AI/ML‑enabled facility concept for pyrochemical nuclear fuel recycling that aims to support a sustainable closed fuel cycle.[web:259] Multimodal sensors feed machine learning models for sensor fusion and anomaly detection, while digital twins automate monitoring and analysis to improve material accountancy, detect variances faster, and reduce false positives in safeguards and process control.[web:259] The technical outputs are intended to enable commercialization and licensing of pyroprocessing facilities that can meet strict regulatory requirements, reduce radiotoxic waste volumes, and economically supply advanced fast reactor fuels.[web:259]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Deep Learning–Based Rapid Tracking of Electromagnetic Radiation Sources Using NaI(Tl) Spectroscopy Detectors",
    "abstract": "This study proposes a deep‑learning‑based system for rapid and accurate localization of electromagnetic radiation sources during emergencies using multiple NaI(Tl) radiation spectroscopy detectors.[web:257] Training data are generated via GATE simulations, and detector coefficients are converted to ratios to compensate for differences between simulation and experimental conditions before being fed into a deep neural network.[web:257] Validated with Cs‑137 sources, the trained model predicts two‑dimensional source coordinates with average positional accuracy of about 95.7%, exceeding 99% in some regions, demonstrating strong potential for real‑time radiation source tracking in emergency response scenarios.[web:257]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Multimodal Learning Enables Instant Ionizing Radiation Alerts on Commodity Mobile Devices",
    "abstract": "This paper presents a multimodal learning approach that turns ordinary mobile phones into practical ionizing‑radiation detectors capable of issuing rapid hazard alerts.[web:260] A hybrid 3D–2D convolutional neural network identifies radiation‑induced spots in raw video frames, while a multilayer perceptron fuses radiation‑signal and brightness maps to estimate dose rates.[web:260] The method detects hazardous dose rates of approximately 25–280 mRem/h within six seconds with 86–96% accuracy, and can detect low‑level radiation around 0.6 mRem/h over longer measurements with about 87% accuracy, indicating strong potential as an accessible radiation‑emergency tool.[web:260]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Can AI Help Fast‑Track Advanced Fuels for Nuclear Reactors?",
    "abstract": "This research effort explores how machine learning can accelerate the design and qualification of advanced nuclear fuels by efficiently coupling with high‑fidelity physics models.[web:252] ML surrogates are trained to approximate expensive fuel performance simulations, enabling rapid exploration of composition and microstructure spaces for safety and efficiency improvements.[web:252] The approach opens opportunities to strengthen reactor safety margins, increase fuel utilization, and shorten development timelines for novel fuel forms and claddings.[web:252]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Nuclear Energy and AI Are Converging to Shape the Future",
    "abstract": "This IAEA statement describes how artificial intelligence is being deployed across the nuclear energy lifecycle in areas such as predictive maintenance, anomaly detection, and thermal performance optimization.[web:265] It highlights applications of AI for plant operations, safety analysis, and design of advanced reactors, while emphasizing the need for robust governance and oversight when integrating AI into safety‑critical domains.[web:265] The piece frames AI as a key enabler for expanding nuclear power’s role in clean energy systems, provided that safety, security, and non‑proliferation requirements are rigorously upheld.[web:265]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  },
  {
    "name": "Nuclear paper",
    "expected_category": "nuclear",
    "title": "Advisory Committee Guidance on the Use of AI in Nuclear Reactor Control and Safety‑Critical Systems",
    "abstract": "This 2025 summary report from a nuclear regulatory advisory committee assesses where AI software is and is not appropriate in nuclear applications.[web:261] It concludes that AI‑based systems, due to unverifiable training algorithms and data, should not be used for closed‑loop real‑time reactor control or other functions where software failures could have severe safety consequences.[web:261] Instead, the report recommends limiting AI to advisory roles, analysis, and monitoring, where traditional deterministic control remains in charge and safety requirements for verifiability and testability can be satisfied.[web:261]",
    "dissemination": "Internal only",
    "audience": "Governance auditor"
  }
]